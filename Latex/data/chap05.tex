\chapter{针对移动相机的实时视频背景减除技术}
在本文第~\ref{ch4:FMCBS}章中提出了一种针对移动相机拍摄视频的快速背景减除算法，该算法能以8帧/秒的速度处理分辨率为$640 \times 480$的视频，而且前景检测准确度达到与当前先进算法接近的水平。虽然第~\ref{ch4:FMCBS}章中提出的快速算法的速度已经比当前领先的一些算法\cite{gbsuperpixel}快几十倍，但是距离实时处理还有一段距离。在一些应用场景下， 如实时监控以及海量监控视频中的移动目标搜索等， 背景减除算法的速度比准确度更重要， 而目前还没有一种算法能实现实时地提取移动相机拍摄视频中较为准确的前景。本章针对实时视频背景减除应用需求，重点研究了实时视频背景减除算法的实现，提出一种简单有效的实时算法。假设视频中大部分区域是背景， 前景只占相对较小部分。 在大多数情况下， 在现实场景中拍摄的视频几乎都满足这个假设。在预处理阶段，首先用基于超像素的区域增长预处理算法得到可能是前景的超像素; 然后基于分块相对光流的背景特征点筛选算法来估算相机运动; 最后检查光流与相机运动的一致性, 得到背景减除的最终结果。 实验结果表明, 该算法可以实时处理大小为$640 \times 480$像素的视频， 且前景检测准确度优于同类实时算法。



\section{相关工作}
\label{ch5:sec:relatedWorks}
 文献~\inlinecite{5.8s}提出一种实时的移动相机下视频背景减除算法。 该算法利用双模式单高斯模型对背景进行建模。使用双模式的高斯模型一方面可以防止前景像素对背景模型产生 干扰，影响背景模型的准确度；另一方面也可以使得背景模型能够适应背景部分像素的变化。此外为了处理相机运动情况，在高斯模型中加入了模型生存时间变量(age)来动态自适应的调整模型更新率。为了加快速度，文献~\inlinecite{5.8s}将输入图像平均分为$ N \times N $ 个均匀网格，每个网格内用一个高斯模型进行背景建模。假设$t$时刻位于网格$i$内的像素集合为$ G_{i}^{(t)}$，网格$i$内的像素个数为$ |G_{i}^{(t)}|$，当观察到像素$ I_j^{(t)}$时，$G_i^{(t)}$内的高斯模型的均值$ \mu_i^{(t)}$，方差$ \sigma_i^{(t)}$ 以及模型生存时间$\alpha_i^{(t)}$ 的更新策略为：
 $$\mu_i^{(t)} = \frac{\hat{\alpha_i^{(t-1)}}}{\hat{\alpha_i^{t-1}}+1}\hat{\mu_i^{(t-1)}} + \frac{1}{\hat{\alpha_i^{(t-1)}}}M_i^{(t-1)}$$

$$\sigma_i^{(t)} = \frac{\hat{\alpha_i^{(t-1)}}}{\hat{\alpha_i^{t-1}}+1}\hat{\sigma_i^{(t-1)}} + \frac{1}{\hat{\alpha_i^{(t-1)}}}V_i^{(t-1)}$$

$$ \alpha_i^{(t) = \hat{\alpha_i^{(t-1)}}}+1$$
其中 $M_i^{(t-1)}$，$V_i^{(t-1)}$分别定义为：
$$ M_i^{(t)} = \frac{1}{|G_i|}\sum_{j\in G_i}I_j^{(t)}$$
$$ V_i^{(t)} = \max_{j \in G_i}{(\mu_i^{(i)})-\mu_j^{(t)}}^2 $$
在文献~\inlinecite{5.8s}算法中，每个网格内同时维护两个高斯模型，其中一个为当前起作用的模型$Model_a$，另一个为候选模型$Model_c$。当观察到当前帧图像与$Model_a$匹配时，利用上述的更新策略对$Model_a$进行更新；若当前帧图像与$Model_c$匹配时，利用同样的更新策略对$Model_c$进行更新。在模型匹配计算前景对象时起作用的一直是$Model_c$，当$Model_c$的生存时间大于$Model_s$的生存时间时，将其两者交换。这种双模式的单高斯模型在效率上比混合高斯模型(GMM)效率更高，且能处理包含动态背景的视频。由于相机是运动的，每次在匹配模型时需要估算相机的运动并进行补偿，返回到静态相机情形。文献~\inlinecite{5.8s}算法利用每个网格点的KLT跟踪结果估算单应性局则很的方式实现。在相机运动补偿后，根据基于像素点和网格中心点的距离的权值，利用周围多个网格高斯模型加权得到像素点未知的高斯模型，通过模型匹配得到像素前景分割结果。由于模型简单，文献~\inlinecite{5.8s}的速度非常快，甚至能在智能手机上实现实时检测。但是文献~\inlinecite{5.8s}没有给出该算法前景检测准确度的定量分析，且实验所用的视频均较容易识别前景。通过实验，作者发现在常用数据库视频的测试中，文献~\inlinecite{5.8s}算法的前景检测准确度较低，而且无法处理相机快速移动的情况。\par

文献~\cite{ACPRRealTime}提出了一种基于像素点轨迹的准实时移动相机视频前景提取算法。该算法处理分辨率为$640 \times 480$的视频的速度约为1.8帧/秒。在针对移动相机的视频背景减除算法中，基于像素点轨迹的方法是主要类型之一。假设在视频中的$F$帧图像中一共有$P$个像素点轨迹，第$i$个像素点的轨迹可以表示为$w_i={[x_{1i}^T \cdots x_{Fi}^{T}]}^T \in \mathbf{R}^{1 \times 2F}$，其中在每一帧图像中像素点的坐标为$ x_{fi} = {[u_{fi},v_{fi}]}^T$。所有这些像素点轨迹组成了一个$2F \times P$的矩阵：
\begin{equation}
\label{ch5:equ:trajmatrix}
\mathbf{W}_{2F \times P} = {[w_{1}^T \cdots w_{P}^T]}^T = \begin{Bmatrix}
u_{11} & \cdots & u_{1P} \\
v_{11} & \cdots  &  v_{1P}\\
 \vdots &  & \vdots \\
 u_{F1}& \cdots & u_{FP}\\
 v_{F1}& \cdots & v_{FP}
\end{Bmatrix}
\end{equation} \par

假设相机成像时满足正交投影，在理想情况下，公式~\ref{ch5:equ:trajmatrix}中的$\mathbf{W}$矩阵的秩应该为3\cite{Tomasi_1992}。由于通过特征点跟踪获取像素点额长期轨迹非常耗时，文献~\cite{ACPRRealTime}提出只利用前一帧图像来获取像素点轨迹，即每个轨迹通过两个特征点的位置只差来计算，第$i$个特征点可以用二维空间向量$\mathbf{v_i}=(dx_i,dy_i) \in \mathbf{R}^2$表示。此外该算法中还利用基于分块的方法获取稀疏的轨迹集合，将图像分为$N \times N$个分块，像素点轨迹矩阵为：
$$ \mathbf{W}_{2 \times (N \times N )} = {[v_{1}^T \cdots v_{N \times N}^T]}^T = \begin{Bmatrix}
dx_1 & \cdots & dx_{N \times N} \\
dy_1 & \cdots & dy_{N \times N}
\end{Bmatrix}$$
利用简化后的矩阵，文献~\cite{ACPRRealTime}算法对轨迹点进行聚类将轨迹点分为背景轨迹点和前景轨迹点两类，最后将轨迹点的标记信息扩散到整幅图像获取每个像素点的前景/背景标记。文献~\cite{ACPRRealTime}的算法在Hopkins数据集中的视频上\cite{HopKinsDataSet}得到的F-Score大约在0.7左右，比文献~\cite{5.8s}算法的准确度高，但是其速度只有不到2帧/秒，远没有达到实时处理。\par

Wang等人~\cite{WangTMM2014}提出了一种多个视频的共同前景提取算法，从一组视频中提取其中共有的前景对象。该算法利用时间连续的超像素算法(temporal superpixels, TSP )对视频中连续的视频帧图像进行分割，将不同视频中的共有前景对象归为同一类。本章提出的算法在流程上与文献~\inlinecite{WangTMM2014}中的算法有相似之处，均是先对视频图像帧进行超像素分割, 然后对超像素进行合并得到前景线索；不同之处是文献~\inlinecite{WangTMM2014}]处理的是提取多个视频中的共同前景问题, 且本章算法所采用的单图像超像素分割算法以及区域合并算法均与文献~\inlinecite{WangTMM2014}有较大区别。 此外，文献~\inlinecite{WangTMM2014}还提出了一种基于Markov随机场的优化过程对结果进行求精， 能准确地提取多个视频中共同的前景，主要用于视频编辑等领域。




\section{算法描述}
\label{ch5:sec:algorithm}
本文算法的流程如图1所示。首先对图像进行超像素分割, 考虑背景像素的连续性, 提出一种基于超像素的区域增长算法对图像进行图像分割预处理。利用背景部分连续性特点，以区域合并的方式不断扩大背景区域的范围。预处理过程结束后得到一个图像区域分割结果，其中大部分背景像素集中在大区域中，属于前景的对象和一些包含边缘和角点的前景处在面积较小的区域中。因此对这部分可能是前景的小区域再进行进一步计算。 为了估算相机运动， 提出一种基于相对光流的特征点筛选算法，筛选出属于背景的特征点， 并利用这些特征点以分块的方式估算相机的运动。 最后通过比较这些超像素的光流与所估算的相机运动的一致性来确定最终的前景。 本文算法简单直接， 不需要建立任何背景模型和计算点轨迹，利用GPU和CPU的混合编程可以实现实时处理。 大量实验证明， 本文算法的前景检测准确度F-Score值达到了0.7左右，远高于同类实时算法\cite{5.8s}。下面对各主要部分进行详细描述.
\subsection{基于超像素的区域增长算法}
在自然场景中所拍摄的视频中, 背景和前景的一个显著区别是背景通常具有连续性, 如天空、树、 墙面、地面等. 这些部分像素的特点是在周围较大的区域内存在与其相似的像素.\par
 如图2所示, 图2a是视频中的一帧图像, 图2b中用不同灰度标注不同背景区域(深色表示树林区域,浅色表示地面区域). 人眼在观察图2a 时, 很快能发现图像中背景部分包括树林和地面等, 这些部分的像素具有明显的连续性; 相反, 作为前景的汽车上的像素则没有这种连续性.\par
 针对这种特点, 受到基于区域增长的图像分割算法[14]的启发, 本文提出一种基于超像素的区域增长算法对待处理图像进行预处理, 得到具有连续性的背景部分. 为了提高效率, 首先对图像进行超像素分割[15]. 采用基于颜色直方图和梯度直方图距离的方式来比较超像素区域之间的相似性
	 	(1)
其中, 是区域之间的颜色直方图的距离, 表示梯度直方图的距离. 本文在RGB色彩空间建立4 096个单元的直方图来描述超像素的颜色特征, 另外利用36个单元的梯度直方图来表示超像素的梯度特征. 采用巴氏距离来计算直方图之间的距离, 式(1)中, c是加权系数, 其值可以通过超像素之间的平均距离来确定,  , 其中Dc, Dg分别表示图像中相邻超像素之间颜色直方图和梯度直方图的平均距离.
算法1. 基于超像素的区域增长算法
输入. 图像I的超像素S， 超像素区域增长门限T.
输出. 区域分割结果R.
Step1.  , 初始化空集C;
Step2. 随机选择一个超像素添加到C;
Step3. While C不为空{
Step4. 从S中取一个超像素s, 将其加入区域R[i];
Setp5. i++, D=0, 初始化空集N
Step6. While D<T {
Step7. 将s四邻域中未访问的超像素加入N;
Step8.  ;
Step9. D=Sd(Sm,R[i]), Sm加入R[i], s=Sm;
Step10. 更新R[i]直方图, 从N中删除Sm;}
Step11. 将N中的超像素添加到C中}.
算法1中, T的值为  图3所示为区域增长算法的一个计算实例. 图3a是输入图像的超像素分割结果, 图3b是区域增长算法的输出结果, 其中用不同的灰度表示不同的区域. 从图3b中可以看到, 该算法能有效地将具有连续性的背景区域超像素合并到一起, 使得这些大区域所包含的超像素数量要明显多于前景区域. 因此本文直接将预处理后超像素数量大于15的区域所包含的超像素判为背景. 对于剩下的超像素, 再用第1.2.3节的算法, 根据其光流与相机运动的一致性来进一步判断其是否为前景.


a. 超像素分割结果         b. 区域增长结果
图3  超像素区域增长结果

\subsection{运动一致性验证}
\label{ch5:sec:sub:motionv}

\subsubsection{特征点筛选}
\label{ch5:sec:sub:sub:fp}
 为了估算相机运动, 首先通过(Kanade-Lucas- Tomasi, KLT)算法[16]对相邻帧进行特征点提取和匹配. 得到匹配特征点后, 需要筛选出属于背景特征点, 并用它们来估算描述相机运动. 在文献[10] 中, 基于相机运动可以用一个全局单应性矩阵描述这一假设, 使用RANSAC 算法来筛选背景特征点. 实际上, 根据多视图几何原理, 在相机自由运动的情况下这一假设并不一定成立[17]. 假设在世界坐标系下, 三维空间场景点用齐次坐标 表示, 为其投影到二维图像中的坐标. 其投影关系为 ; 其中,  是摄像机参数矩阵,  是一个3行4 列的矩阵. 当摄像机有旋转 和平移 时, X 将投影到另一帧图像, 其坐标为 , 这时 与 之间并不存在一对一的映射关系. 考虑以下2种特殊情况:
1) 若摄像机没有平移只有旋转, 有   , 即 与 之间的变换可以用单应性矩阵 表示.
2) 若场景共面, 不失一般性, 假设场景点都位于z=0. 矩阵Q的第3列可以忽略, 因此有
 ,
 ,
 .
即 与 之间的变换可以用单应性矩阵 表示.
当相机运动不满足上述2个假设时, 不能用一个全局单应性矩阵来描述相机运动. 由于RANSAC算法只能用于估算单一模型, 在这种情况下用RANSAC算法筛选出的背景特征点时可能会有较大误差.
考虑到无论相机如何运动, 前景相对于背景都是运动的, 可以利用这种相对的运动区分前景特征点和背景特征点. 假设视频中相邻图像帧 的匹配特征点分别为 , 若已知 中的某个特征点 是背景点, 那么在理想情况下,  中的其他的背景特征点 与 应该是相对静止的; 在 中 与 对应的特征点分别用 和 表示, 这种相对静止的关系可以表示为
	 	(2)
而对于前景特征点 , 因为其相对于背景发生了运动, 其相对光流 并不满足式(2); 其中,  .
若考虑深度等因素对特征点位置的影响， 可能会有一些背景上的特征点并不满足式(2). 而当t取值较小时, 几乎全部前景上的特征点都不会满足式(2). 在估算单应性矩阵时只需要最少4组特征点, 因此在筛选时并不需要得到全部背景特征点, 只需去除位于前景上的特征点. 本文利用这种基于相对光流的思想, 选取满足式(2)的部分背景特征点来估算相机运动. 实验中, 将t的值固定设置为1.0. 背景点A的选择算法为 , 其中,  . 为验证筛选算法的有效性, 定义误检率Pe为pb中前景特征点所占百分比,  .
我们在Hopkins视频数据库[18]中的10个视频上进行了实验, 对本文提出的背景特征点筛选算法与基于RANSAC的背景特征点筛选算法进行了对比. 实验中, 在所有视频中均将2种算法的阈值固定设为1.0. 本文算法得到的Pe平均值为0.32%, 而RANSAC算法筛选的背景特征点Pe平均值为0.36%.
本文提出的特征点算法的一个计算实例如图4所示, 其中, 圆圈标注的为筛选后得到的背景特征点, 方块标注的则为前景特征点.

\subsubsection{}
\label{ch5:sec:sub:sub:homography}
本文提出一种基于分块的单应性矩阵估算算法, 统一处理各种相机运动, 使得图像中不同区域有不同的单应性矩阵. 考虑到分块的连续性, 在计算某分块A的单应性矩阵时, 除了利用分块内的特征点, 还加入了其周围八邻域中已知单应性矩阵的分




图4  基于相对光流的特征点筛选

块N与A的边界点 ,  为N和A中心点之间连线的中点.
算法2. 基于分块的相机运动估算算法
输入. 图像I1 , I2的匹配特征点对F, 网格大小N, 单应性矩阵估算最小特征点数Fmin.
输出. 单应性矩阵数组H.
Step1. 将I1 , I2均匀划分为 个网格
Step2. For i=1: {
Step3. 如果 已知, 继续循环;
Step4. 将以i为中心半径为r=1的区域内的特征点对加入P(i,r);
Step5. 将i八邻域中单应性矩阵已知分块N与i的边界点PB及对应的 加入P(i,r);
Step6. 递增r直到P(i,r)中特征点数量大于Fmin;
Step7. 利用P(i, r)估算 , 区域中各分块 ;}.
在齐次坐标下, 3×3的单应性矩阵有8个未知参数, 因此最少只需要4对匹配特征点就可以估算出矩阵. 为了消除错误匹配特征点带来的误差, 将估算单应性矩阵的最小特征点数设为10. 本文算法假设在图像中背景区域的特征点数要大于这个最小门限, 在绝大部分视频中这个假设很容易满足.


\subsubsection{运动一致性验证}
\label{ch5:sec:sub:sub:motionC}

 对预处理后可能为前景的超像素, 利用KLT计算超像素中心点的光流, 并将其与估算的相机运动进行比较. 假设像素点 位于第i 个分块内, 利用算法2得到的单应性矩阵数组H可以得到变换后的坐标 , 设p点的光流为 . 通过超像素中心点处光流与相机运动的欧几里德距离  来度量这两者的一致性. 当 时, 将该超像素判为前景, 反之为背景. 其中 固定设为2.0.
\section{实验结果与分析}
\label{ch5:sec:results}
 通过GPU和CPU的混合编程, 使用C++语言,借助CUDA和OpenCV实现了本文的算法, 其中, 在GPU上实现了文献[15]中的超像素分割算法, 利用OpenCV提供的GPU实现完成KLT特征点跟踪. 实验中计算机配置为Intel i7 3.5 GHz CPU, NVIDIA Ge-Force Titan GPU. 为了测试算法的有效性, 我们在Hopkins视频数据库中测试了本文算法, 该数据库中的视频包含各种复杂相机运动, 是移动相机背景减除研究中较为常用的测试数据集. 在实验中, 超像素大小约为1 600像素, 在估算相机运动时将图像平均分为8×8 共64个单元的网格, 本文对所有测试视频保持同样的参数.
将本文算法与文献[8,10]中的算法进行了比较, 部分结果如图5所示. 图5a中分别为Hopkins视频



a. 输入图像

b. 前景正确结果

c. 文献[8]算法处理结果

d. 文献[10]算法处理结果

e. 本文算法处理结果
图5  算法前景检测结果及比较

数据库中Cars1视频的第6帧, Cars3视频的第11帧, Cars8视频的第16帧, People1视频的第19帧和Peopel2视频的第16帧. 图5b所示为5a中视频帧的正确前景结果, 图5c所示为文献[8]算法的结果, 图5d所示为文献[10]算法, 图5e所示为本文算法结果. 本文采用通用的前景准确度指标[9]: 精度P, 召回率R和F值(F-score, F)来量化评价算法的准确度, 本文算法与同类实时算法[10]的部分结果如表1所示. 为了实现实时处理, 本文使用了基于超像素的快速区域分割预处理算法, 通过比较可以看出, 在大多数情况下本文算法能有效的检测视频中的前景部分, 而文献[10]的算法在很多情况下得不到满意的结果.
本文算法处理大小为640×480像素的视频的平均速度约为28帧/s, 可以满足实时处理的需要; 文献[10]算法处理同样大小的视频速度要快于本文算法, 约为45帧/s, 但其前景检测准确度较低, 在相机运动幅度较大时算法无法有效工作. 综合前景检测准确度和处理速度, 本文算法更适合实时的实际应用.


\section{本章小结}
\label{ch5:sec:conclusions}
本文提出了一种针对移动相机的实时背景减除算法. 不同于已有的算法, 本文算法不需要计算稠密光流或点轨迹, 也不需要建立并更新背景模型. 根据背景的连续性特点, 通过基于超像素的区域增长算法进行预处理, 先将较为明显的背景去除, 随后根据基于相对光流和分块估计的算法利用匹配特征点估算相机的运动, 对可能的前景超像素进行验证, 比较其光流和相机运动的一致性, 最终获得前景结果. 本文算法简单直接, 能实现实时处理, 大量的实验证明了其有效性, 特别是室外拍摄的背景连续性较强的视频. 相比于同类实时算法, 本文算法的前景检测准确率更高; 而与基于稠密光流或点轨迹的算法相比, 本文算法的处理速度有明显优势.
因为本文算法是建立在前景占视频中较小的部分的假设的, 因此无法处理视频中前景占据主要部分, 或者匹配的特征点集中在前景的情况. 此外, 本文算法不适合处理背景连续性较差的视频, 如室内拍摄视频或包含复杂背景的视频, 针对这些视频, 本文的区域分割算法可能会失效. 在未来的工作中, 我们将考虑解决上述问题, 并在保持算法实时性的前提下, 进一步提高算法的前景检测准确度.
